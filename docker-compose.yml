services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=hdfs-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    ports:
      - "9870:9870" # Web UI
      - "9000:9000" # IPC
    volumes:
      - ./hdfs/data/namenode:/hadoop/dfs/name
      - ./hdfs/config/core-site.xml:/etc/hadoop/core-site.xml
      - ./hdfs/config/hdfs-site.xml:/etc/hadoop/hdfs-site.xml
    networks:
      - app-net
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://namenode:9870" ]
      interval: 10s
      timeout: 5s
      retries: 5
  
  kafka-data:
    container_name: kafka-data
    image: apache/kafka:3.9.0
    hostname: kafka-data
    environment:
      KAFKA_KRAFT_MODE: "true"  # This enables KRaft mode in Kafka.
      KAFKA_PROCESS_ROLES: controller,broker  # Kafka acts as both broker and controller.
      KAFKA_NODE_ID: 1  # A unique ID for this Kafka instance.
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka-data:9093"  # Defines the controller voters.
      KAFKA_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-data:9092
      KAFKA_LOG_DIRS: /var/lib/kafka/data  # Where Kafka stores its logs.
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"  # Kafka will automatically create topics if needed.
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1  # Since weâ€™re running one broker, one replica is enough.
      KAFKA_LOG_RETENTION_HOURS: 168  # Keep logs for 7 days.
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0  # No delay for consumer rebalancing.
    ports:
      - "9092:9092"
      - "9093:9093"
    expose:
      - "9092"
      - "9093"
    networks:
      - app-net




  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - ./hdfs/data/datanode1:/hadoop/dfs/data
      - ./hdfs/config/core-site.xml:/etc/hadoop/core-site.xml
      - ./hdfs/config/hdfs-site.xml:/etc/hadoop/hdfs-site.xml
    networks:
      - app-net
    depends_on:
      namenode:
        condition: service_healthy

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - ./hdfs/data/datanode2:/hadoop/dfs/data
      - ./hdfs/config/core-site.xml:/etc/hadoop/core-site.xml
      - ./hdfs/config/hdfs-site.xml:/etc/hadoop/hdfs-site.xml
    networks:
      - app-net
    depends_on:
      namenode:
        condition: service_healthy


  spark-master:
      image: apache/spark:3.5.4-scala2.12-java17-python3-r-ubuntu
      container_name: spark-master
      command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
      environment:
        - INIT_DAEMON_STEP=setup_spark
        - SPARK_MASTER_MEMORY=4g  # Memory for the Spark Master
        - SPARK_EXECUTOR_MEMORY=2g  # Default executor memory for workers
        - SPARK_DRIVER_MEMORY=2g
        - SPARK_SQL_WINDOW_EXEC_BUFFER_SPILL_THRESHOLD=4194304 # Memory for the driver program
        - SPARK_MEMORY_FRACTION=0.8
        - SPARK_SQL_INMEMORYCOLUMNARSTORAGE_BATCHSIZE=10000
      ports:
        - "8091:8080" # Spark Web UI
        - "7077:7077" # Spark Master
      networks:
        - app-net
      volumes:
        - ./spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      healthcheck:
        test: [ "CMD", "curl", "-f", "http://spark-master:8080" ]
        interval: 10s
        timeout: 5s
        retries: 5
      deploy:
        resources:
          limits:
            memory: 5g  # Total container memory limit
      depends_on:
        namenode:
          condition: service_healthy


  spark-worker1:
      image: apache/spark:3.5.4-scala2.12-java17-python3-r-ubuntu
      container_name: spark-worker1
      environment:
        - SPARK_MASTER=spark://spark-master:7077
        - SPARK_WORKER_MEMORY=4g   # Worker memory
        - SPARK_EXECUTOR_MEMORY=2g
        - SPARK_SQL_WINDOW_EXEC_BUFFER_SPILL_THRESHOLD=4194304# Executor memory
        - SPARK_MEMORY_FRACTION=0.8
        - SPARK_SQL_INMEMORYCOLUMNARSTORAGE_BATCHSIZE=10000

      links:
        - spark-master
      command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
      networks:
        - app-net
      volumes:
        - ./spark/spark-defaults.conf:/spark/conf/spark-defaults.conf
      deploy:
        resources:
          limits:
            memory: 5g
      depends_on:
        spark-master:
          condition: service_healthy
  spark-worker2:
    image: apache/spark:3.5.4-scala2.12-java17-python3-r-ubuntu
    container_name: spark-worker2
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4g   # Worker memory
      - SPARK_EXECUTOR_MEMORY=2g
      - SPARK_SQL_WINDOW_EXEC_BUFFER_SPILL_THRESHOLD=4194304# Executor memory
      - SPARK_MEMORY_FRACTION=0.8
      - SPARK_SQL_INMEMORYCOLUMNARSTORAGE_BATCHSIZE=10000
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    links:
      - spark-master
    networks:
      - app-net
    volumes:
      - ./spark/spark-defaults.conf:/spark/conf/spark-defaults.conf
    deploy:
      resources:
        limits:
          memory: 5g
    depends_on:
      spark-master:
        condition: service_healthy
  app:
    build:
      context: spark/cryptoPredictionApp
      dockerfile: Dockerfile
    container_name: spark-app
#    depends_on:
#      spark-master:
#        condition: service_healthy
##      logstash:
#        condition: service_started

    networks:
      - app-net

  elasticsearch:
      image: docker.elastic.co/elasticsearch/elasticsearch:8.10.2
      container_name: elasticsearch
      environment:
        - discovery.type=single-node
        - bootstrap.memory_lock=true
        - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
        - xpack.security.enabled=false
        - xpack.security.http.ssl.enabled=false
      ulimits:
        memlock:
          soft: -1
          hard: -1
      ports:
        - "9200:9200"
      volumes:
        - ./kibana/logs:/usr/share/elasticsearch/logs/
        - ./kibana/odata:/usr/share/elasticsearch/data
      networks:
        - app-net

  kibana:
        image: docker.elastic.co/kibana/kibana:8.10.2
        container_name: kibana
        ports:
          - "5601:5601"
        environment:
          - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
        depends_on:
          - elasticsearch
        networks:
          - app-net
  logstash:
    image: docker.elastic.co/logstash/logstash:8.7.0
    container_name: logstash
    depends_on:
      - kafka-data
      - elasticsearch
    volumes:
      - ./kibana/logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro
    ports:
      - "5044:5044"
    networks:
      - app-net




networks:
  app-net:
    driver: bridge
