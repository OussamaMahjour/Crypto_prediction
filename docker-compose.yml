services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=hdfs-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    ports:
      - "9870:9870" # Web UI
      - "9000:9000" # IPC
    volumes:
      - ./hdfs/data/namenode:/hadoop/dfs/name
      - ./hdfs/config/core-site.xml:/etc/hadoop/core-site.xml
      - ./hdfs/config/hdfs-site.xml:/etc/hadoop/hdfs-site.xml
    networks:
      - hdfs-net
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://namenode:9870" ]
      interval: 10s
      timeout: 5s
      retries: 5

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - ./hdfs/data/datanode1:/hadoop/dfs/data
      - ./hdfs/config/core-site.xml:/etc/hadoop/core-site.xml
      - ./hdfs/config/hdfs-site.xml:/etc/hadoop/hdfs-site.xml
    networks:
      - hdfs-net
    depends_on:
      namenode:
        condition: service_healthy

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - ./hdfs/data/datanode2:/hadoop/dfs/data
      - ./hdfs/config/core-site.xml:/etc/hadoop/core-site.xml
      - ./hdfs/config/hdfs-site.xml:/etc/hadoop/hdfs-site.xml
    networks:
      - hdfs-net
    depends_on:
      namenode:
        condition: service_healthy


  spark-master:
      image: bde2020/spark-master:latest
      container_name: spark-master
      environment:
        - INIT_DAEMON_STEP=setup_spark
      ports:
        - "8091:8080" # Spark Web UI
        - "7077:7077" # Spark Master
      networks:
        - hdfs-net
      healthcheck:
        test: [ "CMD", "curl", "-f", "http://spark-master:8080" ]
        interval: 10s
        timeout: 5s
        retries: 5
      depends_on:
        namenode:
          condition: service_healthy


  spark-worker:
      image: bde2020/spark-worker:latest
      container_name: spark-worker
      environment:
        - SPARK_MASTER=spark://spark-master:7077
      links:
        - spark-master
      networks:
        - hdfs-net
      depends_on:
        spark-master:
          condition: service_healthy
  app:
    build:
      context: ./spark/data_builder
      dockerfile: Dockerfile
    container_name: spark-app
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - hdfs-net

  elasticsearch:
      image: docker.elastic.co/elasticsearch/elasticsearch:8.10.2
      container_name: elasticsearch
      environment:
        - discovery.type=single-node
        - bootstrap.memory_lock=true
        - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
        - xpack.security.enabled=false
        - xpack.security.http.ssl.enabled=false
      ulimits:
        memlock:
          soft: -1
          hard: -1
      ports:
        - "9200:9200"
      volumes:
        - ./kibana/logs:/usr/share/elasticsearch/logs/
        - ./kibana/odata:/usr/share/elasticsearch/data
      networks:
        - hdfs-net

  kibana:
        image: docker.elastic.co/kibana/kibana:8.10.2
        container_name: kibana
        ports:
          - "5601:5601"
        environment:
          - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
        depends_on:
          - elasticsearch
        networks:
          - hdfs-net





networks:
  hdfs-net:
    driver: bridge
